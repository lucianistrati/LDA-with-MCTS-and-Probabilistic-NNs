# -*- coding: utf-8 -*-
"""LDA_PP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wuUaQwmuf-ejftAq-fh8PmU_jLoBIbQt
"""
# !pip install pymc
from pymc import CompletedDirichlet, Container, Dirichlet, Categorical, Lambda, Model, MCMC

input_data = {
    "fit": [
        {"num_iterations": 50},  # "burn": 30, "thin": 1},
        {"num_iterations": 60},  # "burn": 30, "thin": 1},
        {"num_iterations": 70},  # "burn": 30, "thin": 1},
        {"num_iterations": 80},  # "burn": 30, "thin": 1},
        {"num_iterations": 90},  # "burn": 30, "thin": 1},
        {"num_iterations": 50},  # "burn": 40, "thin": 1},
        {"num_iterations": 60},  # "burn": 40, "thin": 1},
        {"num_iterations": 70},  # "burn": 40, "thin": 1},
        {"num_iterations": 80},  # "burn": 40, "thin": 1},
        {"num_iterations": 90},  # "burn": 40, "thin": 1}
    ],
    "test_to_run": 0,
    "tests": [
        {
            "num_topics": 2,
            "documents": [
                "I had a peanuts butter sandwich for breakfast.",
                "I like to eat almonds, peanuts and walnuts.",
                "My neighbor got a little dog yesterday.",
                "Cats and dogs are mortal enemies.",
                "You mustn't feed peanuts to your dog."
            ]
        },
        {
            "num_topics": 2,
            "documents": [
                "aaa bbb aaa",
                "bbb aaa bbb",
                "aaa bbb bbb aaa",
                "uuu vvv",
                "uuu vvv vvv",
                "uuu vvv vvv uuu"
            ]
        },
        {
            "num_topics": 2,
            "documents": [
                "aaa aaa aaa aaa aaa aaa aaa aaa bbb",
                "bbb bbb bbb bbb bbb bbb bbb bbb bbb bbb bbb bbb",
                "aaa aaa aaa aaa aaa aaa aaa aaa bbb bbb bbb bbb bbb bbb bbb"
            ]
        }
    ]
}

import pdb

import numpy as np
from scipy import spatial

"""
all comments are available for the first example (the one with peanut butter)
"""


class LatentDirichletAllocation:
    def __init__(self, documents, num_topics):
        self.random_state = np.random.RandomState(42)

        self.documents = documents

        self.token_dict = self.get_token_dict()
        self.vectorized_documents = self.token_dict_vectorization()

        self.num_topics = num_topics
        self.num_tokens = len(self.token_dict)
        self.num_documents = len(self.vectorized_documents)

        self.alpha_vec = np.ones(self.num_topics)
        self.beta_vec = np.ones(self.num_tokens)

        self.documents_lengths = [len(document) for document in self.vectorized_documents]

        self.create_lda_vectors()

    def create_lda_vectors(self):
        # topic distribution per-document
        self.theta_vec = Container([CompletedDirichlet("theta1_%s" % i,
                                                       Dirichlet("theta2_%s" % i, theta=self.alpha_vec))
                                    for i in range(self.num_documents)])

        """
        theta_vec - list of len 5 (5 is number of documents) - list of completeddirichlet distributions
        alpha_vec - npy arr of len 2(2 is num of topics)(list of ones at beggining)

        """

        # token distribution per-topic
        self.phi_vec = Container([CompletedDirichlet("phi1_%s" % j,
                                                     Dirichlet("phi2_%s" % j, theta=self.beta_vec))
                                  for j in range(self.num_topics)])

        """
        phi - list of len 2 CompletedDirichlet distributions (num of topics)
        beta_vec - list of len 30 (where 30 is vocab size)(list of ones at beggining)
        """

        #  topic assignments
        self.z_vec = Container([Categorical("z_%i" % d,
                                            p=self.theta_vec[d],
                                            size=self.documents_lengths[d],
                                            value=self.random_state.randint(self.num_topics, size=self.documents_lengths[d]))
                                for d in range(self.num_documents)])

        """
        z - list of len 5 (num of documents), list of Categorical distributions ->
        where each categorical distribution for each doc is obtained with
        the theta_vec CompletedDirichlet distribution of that document, and creates
        a value array with as many elements as tokens in the document having assiged a topic to each token
        """

        # token generated from phi, given a topic z
        self.w_vec = Container([Categorical("w_%i_%i" % (d, i),
                                            p=Lambda("phi_z_%i_%i" % (d, i),
                                                     lambda z=self.z_vec[d][i], phi=self.phi_vec: self.phi_vec[z]),
                                            value=self.vectorized_documents[d][i],
                                            observed=True)
                                for d in range(self.num_documents) for i in range(self.documents_lengths[d])])

        """
        w - list of size 36(where 36 is the number of tokens in the corpus, duplicates allowed) where each element is categorical distribution
        each categorical distribution is obtained from a lambda distribution of each token in each document
        """

        self.pm_model = Model([self.theta_vec, self.phi_vec, self.z_vec, self.w_vec])
        self.mcmc_model = MCMC(self.pm_model)

        """
        self.pm_model.
            containers - matrix of 8 by 5(where 5 - number of documents)
            coparents - set of len 5
            deterministics - set of len 7
            generations - 2 by 5
            markov_blanket - 5 
        """

    def get_token_dict(self):
        token_dict = {}
        token_id = 0
        for document in self.documents:
            tokens = document.split()
            for token in tokens:
                if token not in token_dict:
                    token_dict[token] = token_id
                    token_id += 1
        return token_dict

    def token_dict_vectorization(self):
        vectorized_documents = []
        for document in self.documents:
            tokens = document.split()
            vectorized_document = [self.token_dict.get(token, 0) for token in tokens]
            vectorized_documents.append(vectorized_document)
        return np.array(vectorized_documents)

    def fit(self, num_iterations: int):
        self.mcmc_model.sample(iter=num_iterations)

    def get_topics_distribution_per_document(self):
        topics_distribution_per_document = [self.mcmc_model.trace("theta_%d" % i)[-1] for i in range(self.num_documents)]
        return np.array(topics_distribution_per_document)

    def get_tokens_distribution_per_topic(self):
        tokens_distribution_per_topic = [self.mcmc_model.trace("phi_%d" % i)[-1] for i in range(self.num_topics)]
        return np.array(tokens_distribution_per_topic)

    def get_topics_assignments(self):
        topics_assignments = [self.mcmc_model.trace("z_%d" % i)[-1] for i in range(self.num_documents)]
        return np.array(topics_assignments)

    def get_documents_similarity(self):
        topics_distribution = self.get_topics_distribution_per_document()
        sim_matrix = np.ones((self.num_documents, self.num_documents))
        for i in range(self.num_documents):
            for j in range(self.num_documents):
                if i != j:
                    sim_matrix[i, j] = spatial.distance.cosine(topics_distribution[i], topics_distribution[j])
        return sim_matrix


def run():
    for i in range(len(input_data["fit"])):
        runt_fit_sample(input_data["fit"][i])


def runt_fit_sample(fit_settings):
    lda_model = LatentDirichletAllocation(**input_data["tests"][input_data["test_to_run"]])
    lda_model.fit(**fit_settings)
    # lock.acquire()
    print(str(fit_settings) + "\n")
    get_results(lda_model)
    print(65 * "=" + "\n")
    # lock.release()


def get_results(lda_model):
    print("Vectorized form of the documents")
    for doc in lda_model.vectorized_documents:
        print(doc)
    print()

    print("Topics distribution per document")
    topics_distribution_per_document = lda_model.get_topics_distribution_per_document()
    for document_topics_distribution in topics_distribution_per_document:
        print(document_topics_distribution)
    print()

    print("tokens distribution per topic")
    tokens_distribution_per_topic = lda_model.get_tokens_distribution_per_topic()
    for topic_tokens_distribution in tokens_distribution_per_topic:
        print(topic_tokens_distribution)
    print()

    print("Topics assignments")
    topics_assignments = lda_model.get_topics_assignments()
    for document_topics_assignment in topics_assignments:
        print(document_topics_assignment)
    print()

    print("documents similarity")
    documents_similarity = lda_model.get_documents_similarity()
    for document_similarity in documents_similarity:
        print(document_similarity)
    print()


a = [0.00813817, 0.0220345, 0.07021902, 0.05891476, 0.01771101, 0.03078813, 0.03638544, 0.08070292, 0.01071666, 0.09154098, 0.00655607, 0.05660983, 0.01259865, 0.07081327, 0.03445258, 0.02569094, 0.13147004, 0.01742931, 0.02532453, 0.01163437, 0.06359029, 0.00767481, 0.01630882, 0.00990558, 0.0104397, 0.0026773, 0.01819213, 0.01741026, 0.03051626, 0.00355356]

print(sum(a))
# run()




